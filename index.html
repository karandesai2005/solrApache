<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Solr Apache Documentation</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      line-height:1.6;
    }

    header {
      background-color: #333;
      color: #fff;
      padding: 20px;
      text-align: center;
    }

    nav ul {
      list-style-type: none;
      margin: 0;
      padding: 0;
      background-color: #f1f1f1;
      text-align: center;
    }

    nav ul li {
      display: inline;
    }

    nav ul li a {
      display: block;
      padding: 10px;
      text-decoration: none;
      color: #333;
    }

    nav ul li a:hover {
      background-color: #ddd;
    }

    main {
      padding: 20px;
    }

    section {
      margin-bottom: 20px;
    }

    footer {
      background-color: #333;
      color: #fff;
      padding: 10px;
      text-align: center;
      font-size: 14px;
    }
   .top-arrow {
  position: fixed;
  bottom: 20px;
  right: 20px;
  background-color: #f0f0f0;
  color: #000000;
  font-size: 24px;
  padding: 10px;
  text-decoration: none;
  border-radius: 50%;
}
.top-arrow:hover {
  background-color: #cccccc;
}
  </style>
</head>
<body>
<header>
  <h1>Solr Apache Documentation</h1>
</header>

<nav>
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#indexing">Indexing</a></li>
    <li><a href="#search">Search</a></li>
    <li><a href="#Tokenizers">Tokenizers</a></li>
  </ul>
</nav>

<main>
  <section id="introduction">
    <h2>Introduction</h2>
     <p>SOLR: (stands for Searching On Lucene w/ Replication). Solr is an open-source, enterprise search platform that can be used to index and search large amounts of data. It is built on top of Apache Lucene, which is a popular full-text search library. Solr provides several features that make it a powerful tool for search and analytics, including:</p>
  <ul>
    <li>Flexible schema: Solr allows you to define your schema, making it easy to index and search different data types.</li>
    <li>Scalability: Solr can be scaled to handle large amounts of data.</li>
    <li>High availability: Solr is designed to be highly available, so your search applications will be up and running even if there are problems with individual servers.</li>
    <li>Performance: Solr is very fast so that you can search large datasets quickly.</li>
    <li>Ease of use: Solr is easy to use and manage.</li>
  </ul>
  <p>Solr is used by a wide variety of organizations, including:</p>
  <ul>
    <li>E-commerce websites: Solr can be used to index product catalogs, so customers can easily search for products. In e-commerce, Solr is used to index product data, such as product names, descriptions, prices, and images. This allows users to search for products by keyword, category, price, or other criteria. Solr also supports faceting, which allows users to filter search results by different criteria, such as brand, color, or size.</li>
    <li>News websites: Solr can be used to index articles, so readers can easily find the information they are looking for.</li>
    <li>Government agencies: Solr can be used to index documents, so citizens can easily find information about government programs and services.</li>
    <li>Businesses: Solr can be used to index customer data, so businesses can better understand their customers and target them with marketing campaigns.</li>
  </ul>
  <p>Here are a few companies that were known to use Apache Solr as of September 2021:</p>
  <ol>
    <li>Netflix: Netflix has been a long-time user of Solr for various search and recommendation functionalities within its streaming platform.</li>
    <li>Apple: Apple has utilized Solr in its support communities to power search and facilitate efficient retrieval of relevant information.</li>
    <li>eBay: eBay has employed Solr to power search and navigation features on its online marketplace, enabling users to find and explore products efficiently.</li>
    <li>Zillow: Zillow, a prominent real estate marketplace, has utilized Solr to power its search functionality, allowing users to find properties and relevant information.</li>
    <li>Instagram: Instagram, a popular social media platform, has utilized Solr for search and retrieval functionalities, enabling users to find posts, users, and relevant content.</li>
  </ol>
  <p>So, for example, Solr acts as an index for a book, helping users find specific words or phrases by providing the corresponding page numbers. Similarly, Solr is a search engine that indexes a set of documents (e.g., news articles) and allows users to query Solr to retrieve a set of documents that match their search query.</p>

  </section>

  <section id="installation">
    <h2>Installation</h2>
  <p>Open the terminal in Ubuntu in the sudo version</p>
  <p>Apache Solr 7 requires Java 8 or greater to run. Make sure your system fulfills the Java requirements of Apache Solr.</p>
  <code>sudo apt install openjdk-11-jdk</code>
  <p>Verify active Java version:</p>
  <code>java -version</code>
  <pre>
    openjdk version "11.0.4" 2019-07-16
    OpenJDK Runtime Environment (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3)
    OpenJDK 64-Bit Server VM (build 11.0.4+11-post-Ubuntu-1ubuntu218.04.3, mixed mode, sharing)
  </pre>
  <p>Now download the required Solr version from its official site or Mirror. Or simply use the following command to download Apache Solr 9.0.</p>
  <code>Wget https://dlcdn.apache.org/solr/solr/9.2.1/solr-9.2.1.tgz</code>
  <p>Now extract the Apache Solr service installer shell script from the downloaded Solr archive file and run the installer using the following commands.</p>
  <code>tar xzf solr-9.0.0.tgz</code>
  <p>Step 3 â€“ Start / Stop Solr Service</p>
  <p>Solr is configured as a service on your system. You can simply use the following commands to Start, Stop and check the status of the Solr service.</p>
  <code>sudo bin/solr stop</code>
  <code>sudo bin/solr start</code>
  <code>sudo bin/solr status</code>
  <p>Create First Solr Collection</p>
  <p>After the successful installation of Solr on your system. Create the first collection in Apache Solr using the following command.</p>
  <code>bin/solr create -c mycore</code>
  <p>Sample output:</p>
  <pre>Created new core 'mycol1'</pre>
  <p>To verify the installation, use the following URL in your browser: <a href="http://localhost:8983/">http://localhost:8983/</a></p>

  </section>

  <section id="configuration">
    <h2>Configuration</h2>
    <h4>Apache Solr Schema</h4>
  <p>The <code>schema.xml</code> file in Apache Solr defines the schema for indexing and searching data. It specifies the fields, field types, and other configurations for the Solr collection.</p>
  <xmp><code>
    <?xml version="1.0" encoding="UTF-8" ?>
    <schema name="example" version="1.6">
      <!-- Fields -->
      <fields>
        <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
        <field name="name" type="text_general" indexed="true" stored="true" />
        <field name="description" type="text_general" indexed="true" stored="true" />
        <field name="school" type="text_general" indexed="true" stored="true"/>
        <field name="address" type="text_general" indexed="true" stored="true"/>
        <field name="location" type="location" indexed="true" stored="true"/>
        <dynamicField name="*_coordinate" type="location" indexed="true" stored="true"/>
        
        <!-- Copy Fields -->
        <field name="copyField" type="text_general" indexed="true" stored="true"/>
        <copyField source="name" dest="copyField"/>
        <copyField source="description" dest="copyField"/>
        <copyField source="school" dest="copyField"/>
        <copyField source="address" dest="copyField"/>
        <copyField source="location" dest="copyField"/>
      </fields>
      
      <!-- Unique Key and Default Search Field -->
      <uniqueKey>id</uniqueKey>
      <defaultSearchField>name</defaultSearchField>
      
      <!-- Solr Query Parser Configuration -->
      <solrQueryParser defaultOperator="OR" />
      
      <!-- Field Types -->
      <types>
        <fieldType name="string" class="solr.StrField" />
        <fieldType name="text_general" class="solr.TextField" positionIncrementGap="100">
          <analyzer type="index">
            <tokenizer class="solr.StandardTokenizerFactory" />
            <filter class="solr.LowerCaseFilterFactory" />	
          </analyzer>
          <analyzer type="query">
            <tokenizer class="solr.StandardTokenizerFactory" />
            <filter class="solr.LowerCaseFilterFactory" />
            <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
          </analyzer>
        </fieldType>
        <fieldType name="location" class="solr.LatLonPointSpatialField" spatialContextFactory="com.spatial4j.core.context.jts.JtsSpatialContextFactory" autoIndex="true" />
      </types>
    </schema>
  </code></xmp>

  <p>The solrconfig.xml file in Apache Solr is the main configuration file that controls the behavior and settings of the Solr server. It defines various components and handlers responsible for indexing, searching, and managing the Solr instance.</p>
  
  <h4>solrconfig.xml</h4>
<xmp>
    <?xml version="1.0" encoding="UTF-8" ?>
    <config>
      <!-- Libraries -->
      <lib dir="${solr.install.dir:../../../..}/dist/" regex="solr-spatial-\d.*\.jar" />
      <lib dir="${solr.install.dir:../../../..}/contrib/extraction/lib" regex=".*\.jar" />
      <!-- Lucene Match Version -->
      <luceneMatchVersion>9.2.1</luceneMatchVersion>
      <!-- Update Handler -->
      <updateHandler class="solr.DirectUpdateHandler2">
        <updateLog/>
      </updateHandler>

      <!-- Spatial Configuration -->
      <spatial>
        <fieldType name="location" class="solr.LatLonPointSpatialField" geo="true" />
        <spatialFilterCache class="solr.LRUCache" size="10000" initialSize="1000"/>
        <spatialQueryCache class="solr.LRUCache" size="10000" initialSize="1000"/>
        <spatialStrategy name="location" class="solr.SpatialRecursivePrefixTreeStrategy" field="location"/>
      </spatial>
      <!-- Query Settings -->
      <query>
        <useFilterForSortedQuery>true</useFilterForSortedQuery>
        <queryResultWindowSize>100</queryResultWindowSize>
        <queryResultMaxDocsCached>100</queryResultMaxDocsCached>
        <useColdSearcher>false</useColdSearcher>
      </query>
      <!-- Request Dispatcher -->
      <requestDispatcher handleSelect="true">
        <requestParsers enableRemoteStreaming="true" multipartUploadLimitInKB="2048000" />
      </requestDispatcher>
      <!-- Request Handlers -->
      <requestHandler name="/select" class="solr.SearchHandler">
        <lst name="defaults">
          <str name="echoParams">explicit</str>
          <int name="rows">10</int>
          <str name="df">name</str>
          <str name="defType">edismax</str>
        </lst>
      </requestHandler>
      <requestHandler name="/update" class="solr.UpdateRequestHandler" />
      <!-- Search Components -->
      <searchComponent name="spellcheck" class="solr.SpellCheckComponent">
        <!-- Spellchecker Configuration -->
        <str name="queryAnalyzerFieldType">text_general</str>
        <lst name="spellchecker">
          <str name="name">default</str>
          <str name="field">name</str>
          <str name="classname">solr.DirectSolrSpellChecker</str>
          <str name="distanceMeasure">internal</str>
          <float name="accuracy">0.5</float>
          <int name="maxEdits">2</int>
          <int name="minPrefix">1</int>
          <int name="maxInspections">5</int>
          <int name="minQueryLength">4</int>
          <float name="maxQueryFrequency">0.01</float>
        </lst>
      </searchComponent>
      <searchComponent name="spatial" class="solr.SpatialComponent"/>
      <!-- Request Handlers -->
      <requestHandler name="/spell" class="solr.SearchHandler" startup="lazy">
        <lst name="defaults">
          <str name="df">name</str>
          <str name="spellcheck.dictionary">default</str>
          <str name="spellcheck">on</str>
          <str name="spellcheck.extendedResults">false</str>
          <str name="spellcheck.count">5</str>
        </lst>
        <arr name="last-components">
          <str>spellcheck</str>
        </arr>
      </requestHandler>
      <admin>
        <defaultQuery>*:*</defaultQuery>
      </admin>
      <!-- Default Request Handler -->
      <defaultRequestHandler name="standard" class="solr.SearchHandler" default="true">
        <lst name="defaults">
          <str name="echoParams">explicit</str>
          <int name="rows">10</int>
          <str name="df">name</str>
          <str name="defType">edismax</str>
        </lst>
      </defaultRequestHandler>
      <!-- Update Request Processor Chain -->
      <updateRequestProcessorChain name="partial-update">
        <processor class="solr.AddSchemaFieldsUpdateProcessorFactory">
          <str name="defaultFieldType">text_general</str>
          <lst name="fieldMappings">
            <str name="name">copyField</str>
            <str name="description">copyField</str>
            <str name="school">copyField</str>
            <str name="address">copyField</str>
            <str name="location">copyField</str>
          </lst>
        </processor>
        <processor class="solr.LogUpdateProcessorFactory" />
        <processor class="solr.RunUpdateProcessorFactory" />
      </updateRequestProcessorChain>
      <types>
        <!-- Other field type definitions -->
        <fieldType name="location" class="solr.SpatialRecursivePrefixTreeFieldType" geo="true" distErrPct="0.025" maxDistErr="0.001" distanceUnits="kilometers" />
      </types>  

      <requestHandler name="/partial-update" class="solr.UpdateRequestHandler" >
        <lst name="defaults">
          <str name="update.chain">partial-update</str>
        </lst>
      </requestHandler>
    </config>
  </xmp>
  </section>

  <section id="indexing">
    <h2>Indexing</h2>
  <p>Python can be used to index data into Apache Solr, allowing you to populate your Solr index with data from various sources, such as databases, APIs, or local files.</p>
  
  <h4>Python Code Example</h4>
  <pre><code>
    import pymysql
    import json
    import pysolr

    # Establish database connection
    host = 'localhost'
    username = 'root'
    password = ''
    database = 'db1'
    connection = pymysql.connect(host=host, user=username, password=password, db=database)
    cursor = connection.cursor()

    # Fetch data from the database
    query = "SELECT * FROM sampl2"
    cursor.execute(query)
    rows = cursor.fetchall()

    # Close the database connection
    connection.close()

    # Convert rows to a list of dictionaries
    data = []
    for row in rows:
        dict_row = {
            "id": row[0],
            "name": row[1],
            "description": row[2],
            "school": row[3],
            "address": row[4],
            "location": row[5]
        }
        data.append(dict_row)

    # Convert data to JSON string
    json_data = json.dumps(data, indent=4)

    # Write JSON data to a file
    file_path = 'data.json'
    with open(file_path, 'w') as file:
        file.write(json_data)

    # Connect to Solr
    solr = pysolr.Solr('http://localhost:8983/solr/mycore')

    # Index the JSON data in Solr
    solr.add(data)

    # Commit the changes to make them visible in the index
    solr.commit()
  </code></pre>
  
  <p>This Python code demonstrates how to index data into Apache Solr using the <code>pysolr</code> library.</p>
  <ol>
    <li>First, the code establishes a connection to a MySQL database and fetches data from the <code>sampl2</code> table.</li>
    <li>Next, it converts the fetched data into a list of dictionaries, where each dictionary represents a document to be indexed.</li>
    <li>The data is then converted to a JSON string and written to a file named <code>data.json</code>.</li>
    <li>The code connects to the Solr instance running at <code>http://localhost:8983/solr/mycore</code>.</li>
    <li>The JSON data is indexed in Solr using the <code>solr.add()</code> method.</li>
    <li>Finally, the changes are committed to make them visible in the Solr index using <code>solr.commit()</code>.</li>
  </ol>
  </section>

  <section id="search">
    <h2>Search</h2>
    <p>
      this is the json file we indexed
      <xmp>
      [
	{
    	"id": 1,
    	"name": "karan",
    	"description": "hello my name is karan i am 18",
    	"school": "sncs",
    	"address": "newyork",
    	"location": "51.507400,-0.127800"
	},
	{
    	"id": 2,
    	"name": "ishaan",
    	"description": "hello my name is ishaam i am 13",
    	"school": "sncs",
    	"address": "london",
    	"location": "51.507400,-0.127800"
	},
	{
    	"id": 3,
    	"name": "harsh",
    	"description": "hello my name is harsh i am 19",
    	"school": "navkar",
    	"address": "newjercy",
    	"location": "40.707937,-74.041602"
	},
	{
    	"id": 4,
    	"name": "aarsh",
    	"description": "my name is Aarsh i am 18",
    	"school": "sas",
    	"address": "london_4km",
    	"location": "51.542859,-0.133613"
	}
]
    </xmp>
  <h4>Example Search Queries in Apache Solr</h4>
  <p>Here are some example search queries you can use to retrieve data from the indexed documents:</p>

  <ol>
    <li>
      <p>Search by name:</p>
      <pre><code>q=name:karan</code></pre>
      <p>This query will retrieve documents where the "name" field matches the term "karan".</p>
    </li>

    <li>
      <p>Search by description:</p>
      <pre><code>q=description:hello</code></pre>
      <p>This query will retrieve documents where the "description" field contains the term "hello".</p>
    </li>

    <li>
      <p>Search by school:</p>
      <pre><code>q=school:sncs</code></pre>
      <p>This query will retrieve documents where the "school" field matches the term "sncs".</p>
    </li>

    <li>
      <p>Search by address:</p>
      <pre><code>q=address:london</code></pre>
      <p>This query will retrieve documents where the "address" field matches the term "london".</p>
    </li>

    <li>
      <p>Spatial search:</p>
      <pre><code>q={!geofilt pt=51.507400,-0.127800 sfield=location d=10}</code></pre>
      <p>This query will retrieve documents within a 10-kilometer radius of the coordinates (51.507400, -0.127800) specified in the "location" field.</p>
    </li>
  </ol>

  <p>Note that the queries assume you are searching within the "mycore" Solr core. If you have a different core name, replace "mycore" with the actual name of your core in the query URLs.</p>

  <p>You can customize these queries further based on your specific requirements and the fields you want to search on.</p>

    </p>
  </section>
 <section id="Tokenizers">
	 Certainly! Here's the HTML code with the provided text added:

html
Copy code
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Solr Apache Documentation</title>
  <style>
    /* CSS styles here */
  </style>
</head>
<body>
<header>
  <h1>Solr Apache Documentation</h1>
</header>

<nav>
  <a href="#" class="menu-toggle">Menu</a>
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#indexing">Indexing</a></li>
    <li><a href="#search">Search</a></li>
  </ul>
</nav>

<main>
  <section id="introduction">
    <h2>Introduction</h2>
    <p>SOLR: (stands for Searching On Lucene w/ Replication). Solr is an open-source, enterprise search platform that can be used to index and search large amounts of data. It is built on top of Apache Lucene.</p>
  </section>

  <section>
    <h2>Tokenizers in Apache Solr</h2>
    <p>Tokenizers are tools or libraries used in natural language processing (NLP) to break down text into smaller units called tokens. These tokens can be words, subwords, characters, or any other meaningful subunits, depending on the type of tokenizer used.</p>
    <p>Tokenization is an essential step in many NLP tasks, such as text classification, named entity recognition, machine translation, and sentiment analysis. By splitting text into tokens, it becomes easier to process and analyze language data.</p>
    <p>In Apache Solr, tokenizers are components of the text analysis pipeline used for indexing and querying text data. Solr provides various built-in tokenizers that can be configured in the schema.xml file or through the Solr admin interface.</p>
    <p>Here are some commonly used tokenizers in Solr:</p>
    <ol>
      <li>
        <strong>StandardTokenizer:</strong> This tokenizer divides text into tokens based on word boundaries and punctuation. It follows the Unicode Text Segmentation algorithm and is suitable for most Western languages.
      </li>
      <li>
        <strong>ClassicTokenizer:</strong> Similar to the StandardTokenizer, the ClassicTokenizer tokenizes text based on word boundaries and punctuation. However, it also handles certain language-specific rules, such as splitting "don't" into "do" and "n't".
      </li>
      <li>
        <strong>WhitespaceTokenizer:</strong> This tokenizer splits text into tokens based on whitespace characters. It doesn't handle punctuation or language-specific rules, but it can be useful in specific scenarios where you want to preserve whitespace as a token delimiter.
      </li>
      <li>
        <strong>KeywordTokenizer:</strong> This tokenizer treats the entire input as a single token. It can be useful when you want to index or search for exact phrases or specific terms without any tokenization.
      </li>
      <li>
        <strong>PatternTokenizer:</strong> This tokenizer uses a regular expression pattern to split the text into tokens. You can define custom patterns to match specific token boundaries based on your requirements.
      </li>
    </ol>
    <p>Certainly! Let's take an example to illustrate the usage of tokenizers in Apache Solr.</p>
    <p>Suppose you have a collection of product descriptions, and you want to index them in Solr to enable searching and retrieval. Here's a simplified schema for our example:</p>
    <pre>
      &lt;schema&gt;
        &lt;fieldType name="text_general" class="solr.TextField" positionIncrementGap="100"&gt;
          &lt;analyzer&gt;
            &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
            &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
            &lt;filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/&gt;
          &lt;/analyzer&gt;
        &lt;/fieldType&gt;

        &lt;field name="description" type="text_general" indexed="true" stored="true"/&gt;
      &lt;/schema&gt;
    </pre>
    <p>In this schema, we're using the <code>text_general</code> field type, which employs the <code>StandardTokenizer</code> for tokenization. It is followed by the <code>LowerCaseFilter</code> to convert all tokens to lowercase and the <code>StopFilter</code> to remove common English stopwords.</p>
    <p>Now, let's consider a sample product description: "The latest smartphone features a stunning 6.5-inch display and a powerful quad-core processor."</p>
    <p>When this description is indexed in Solr, the tokenizer and filters will process the text to generate tokens. The tokens produced after each step of the analysis are as follows:</p>
    <ol>
      <li>
        <strong>Tokenization (StandardTokenizer):</strong>
        <ul>
          <li>Tokens: ["The", "latest", "smartphone", "features", "a", "stunning", "6.5-inch", "display", "and", "a", "powerful", "quad-core", "processor"]</li>
        </ul>
      </li>
      <li>
        <strong>Lowercasing (LowerCaseFilter):</strong>
        <ul>
          <li>Tokens: ["the", "latest", "smartphone", "features", "a", "stunning", "6.5-inch", "display", "and", "a", "powerful", "quad-core", "processor"]</li>
        </ul>
      </li>
      <li>
        <strong>Stopword removal (StopFilter):</strong>
        <ul>
          <li>Tokens: ["latest", "smartphone", "features", "stunning", "6.5-inch", "display", "powerful", "quad-core", "processor"]</li>
        </ul>
      </li>
    </ol>
    <p>After the analysis process, these tokens are stored in the Solr index and can be searched using various queries. For example, if a user searches for "smartphone features," Solr will match documents that contain the tokens "smartphone" and "features" in proximity.</p>
    <p>This example demonstrates how the tokenizer and filters in Solr's text analysis pipeline can transform the input text into meaningful tokens that are suitable for indexing and searching. By configuring different tokenizers and filters based on your specific requirements, you can customize the tokenization process to enhance the search experience and relevance in your Solr application.</p>
  
 </section>
</main>

<footer>
  <p>&copy; 2023 Solr Apache Documentation. All rights reserved.</p>
</footer>
<a href="#" class="top-arrow">&#8593;</a>

</body>
</html>
